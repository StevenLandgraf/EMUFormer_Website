<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

<!-- Meta tags for search engines and social media -->
<meta name="description" content="EMUFormer is a novel student-teacher distillation method for efficient multi-task uncertainty estimation in joint semantic segmentation and monocular depth estimation. It achieves high-quality uncertainty with low inference cost.">
<meta name="keywords" content="multi-task learning, semantic segmentation, monocular depth estimation, uncertainty estimation, student-teacher distillation, deep ensemble, calibration, EMUFormer, Cityscapes, NYUv2, safety-critical AI, computer vision">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Open Graph (Facebook, LinkedIn, etc.) -->
<meta property="og:title" content="Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation" />
<meta property="og:description" content="EMUFormer is a novel student-teacher distillation approach for estimating efficient, high-quality uncertainties in joint semantic segmentation and monocular depth estimation." />
<meta property="og:url" content="https://stevenlandgraf.github.io/EMUFormer_Website/" />
<meta property="og:image" content="static/images/icon.png" />
<meta property="og:image:width" content="512" />
<meta property="og:image:height" content="512" />
<meta property="og:type" content="website" />

  <title>Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7DOqcXkAAAAJ&hl=en" target="_blank">Steven Landgraf</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=_qfDWfUAAAAJ&hl=en" target="_blank">Markus Hillemann</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/theodor-kapler/" target="_blank">Theodor Kapler</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=JutLoKsAAAAJ&hl=en" target="_blank">Markus Ulrich</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Machine Vision Metrology (MVM)<br>Institute of Photogrammetry and Remote Sensing (IPF)<br>Karlsruhe Institute of Technology (KIT)<br> DAGM German Conference on Pattern Recognition (GCPR, 2024)<br>Best Paper Award Honorable Mention</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- PDF link -->
                      <span class="link-block">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-85187-2_22" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2402.10580" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/StevenLandgraf/U-CE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence, lack of explainability, and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving or robotics, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. To this end, we introduce EMUFormer, a novel student-teacher distillation approach for efficient multi-task uncertainties in the context of joint semantic segmentation and monocular depth estimation. By leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Demo Videos</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/video_3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Methodology Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Methodology</h2>
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="static/images/methodology.png" alt="U-CE Method Overview" style="max-width: 100%; height: auto;" />
        <div class="content has-text-justified">
          <p>
            A schematic overview of EMUFormer. In addition to the regular CE loss for the SS task and the GNLL loss, EMUFormer utilizes two additional losses
            that distill the predictive uncertainties of the teacher into the student model.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Quantitative Results Section -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Comparison against State of the Art</h2>
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <img src="static/images/comparison_sota.png" style="max-width: 100%; height: auto;" />
        <div class="content has-text-justified">
          <p>
            EMUFormer-B5 surpasses the previous state of the art in joint SS and MDE on both Cityscapes [39] and NYUv2 [245]. For instance, on
            NYUv2 [245], it achieves 1.4 % higher mIoU and 0.007 lower RMSE than MTFormer [287],
            which also adopts a modern ViT-based architecture. In contrast to our work, however, MTFormer
            relies on cross-task attention and a complex self-supervised pre-training pipeline,
            which introduces additional complexity. Moreover, EMUFormer yields high-quality uncertainty
            estimates without any additional computational overhead during inference.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Qualitative Results (in-Domain)</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/qualitative_results_1.png" alt="MY ALT TEXT" style="max-width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of EMUFormer-B2 on Cityscapes.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qualitative_results_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of EMUFormer-B2 on NYUv2.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Qualitative Results (Out-of-Domain)</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/ood_qualitative_results_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of EMUFormer-B2 on Foggy Cityscapes.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/ood_qualitative_results_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of EMUFormer-B2 on Rainy NYUv2.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Paper Conclusion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
          EMUFormer employs student-teacher distillation to achieve state-of-the-art results in joint semantic segmentation and monocular depth estimation on Cityscapes [6] and NYUv2 [54]. Simultaneously, it estimates well-calibrated predictive uncertainties for both tasks. This is achieved without introducing any additional computational overhead during inference, making EMUFormer usable for time-critical applications. EMUFormer even surpasses the performance of its DE teacher in certain cases, despite the latter having ten times the parameters and approximately 30 times higher inference time. Most interestingly, EMUFormer achieves particularly outstanding performance in the depth estimation task in comparison to the teacher. This success can be primarily attributed to the use of the Gaussian Negative Log-Likelihood loss (cf. Sect. 5.3), which is commonly employed to implicitly learn corresponding variances in addition to the predictive means. In the case of EMUFormer, however, the teacher model already provides high-quality variances through distillation, allowing for a more accurate approximation of the predictive means and their associated uncertainties. Overall, these findings go along nicely with previous work [28, 32] on leveraging uncertainties during training, making it an interesting venue for future work.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper conclusion -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{landgraf2024efficient,
  title={Efficient multi-task uncertainties for joint semantic segmentation and monocular depth estimation},
  author={Landgraf, Steven and Hillemann, Markus and Kapler, Theodor and Ulrich, Markus},
  booktitle={DAGM German Conference on Pattern Recognition},
  pages={348--364},
  year={2024},
  organization={Springer}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
